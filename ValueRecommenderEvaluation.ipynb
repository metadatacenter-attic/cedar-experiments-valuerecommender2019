{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Using association rule mining and ontologies to generate metadata recommendations from multiple biomedical databases_\n",
    "\n",
    "**Authors**: [Marcos Martínez-Romero](https://orcid.org/0000-0002-9814-3258)*, [Martin J. O'Connor](https://orcid.org/0000-0002-2256-2421), [Attila L. Egyedi](https://orcid.org/0000-0003-0730-5053), [Debra Willrett](https://orcid.org/0000-0002-3767-2957), [Josef Hardi](https://orcid.org/0000-0002-2533-6681), \n",
    "[John Graybeal](https://orcid.org/0000-0001-6875-5360), and [Mark A. Musen](https://orcid.org/0000-0003-3325-793X). \n",
    "\n",
    "**Affiliation**: [Stanford Center for Biomedical Informatics Research](https://bmir.stanford.edu/), 1265 Welch Road, Stanford University School of Medicine, Stanford, CA 94305-5479, USA\n",
    "\n",
    "\\* Corresponding author\n",
    "\n",
    "**Paper status**: submitted (under review)\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose of this document\n",
    "\n",
    "This document is a [Jupyter notebook](http://jupyter.org/) that describes all the steps to reproduce the evaluation presented in our paper. All the source code used to generate the results and figures in the paper is in the [scripts folder](./scripts). The results generated by the code will be saved to a local `workspace` folder. All the data and results generated by our experiments can be downloaded as a .zip file [from here](https://drive.google.com/open?id=1X8-K1DjRh4FAmRKuGed1XXKsA1iOSl5x). \n",
    "\n",
    "\n",
    "## Table of contents\n",
    "* [Step 1. Datasets download](#s1)\n",
    "    * [1.a. NCBI BioSample](#s1-a)\n",
    "    * [1.b. EBI BioSamples](#s1-b)\n",
    "* [Step 2: Generation of template instances](#s2)\n",
    "    * [2.1. Determine relevant attributes and create CEDAR templates](#s2-1)\n",
    "        * [2.1.a. NCBI BioSample](#s2-1-a)\n",
    "        * [2.1.b. EBI BioSamples](#s2-1-b)\n",
    "    * [2.2. Select samples](#s2-2)\n",
    "        * [2.2.a. NCBI BioSample](#s2-2-a)\n",
    "        * [2.2.b. EBI BioSamples](#s2-2-b)\n",
    "    * [2.3. Generate CEDAR instances](#s2-3)\n",
    "* [Step 3: Semantic annotation](#s3)\n",
    "    * [3.1. Extraction of unique values from CEDAR instances](#s3-1)\n",
    "    * [3.2. Annotation of unique values and generation of mappings](#s3-2)\n",
    "    * [3.3. Annotation of CEDAR instances](#s3-3)\n",
    "* [Step 4: Generation of experimental data sets](#s4)\n",
    "* [Step 5: Training](#s5)\n",
    "    * [5.1. Rules generated](#s5-results)\n",
    "* [Step 6: Testing](#s6)\n",
    "* [Step 7: Analysis of results](#s7)\n",
    "* [Additional experiments](#additional-experiments)\n",
    "    * [Additional experiment 1](#additional-experiment-1)\n",
    "    * [Additional experiment 2](#additional-experiment-2)\n",
    "* [Links](#links)\n",
    "* [Contact](#contact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download\n",
    "Download the data.zip file from https://drive.google.com/a/stanford.edu/file/d/1X8-K1DjRh4FAmRKuGed1XXKsA1iOSl5x/view?usp=sharing and uncompress it in your repository root folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s1\"></a>Step 1: Datasets download\n",
    "### <a name=\"s1-a\"></a>1.a. NCBI BioSample\n",
    "We downloaded the full content of the [NCBI BioSample database](https://www.ncbi.nlm.nih.gov/biosample/) from the [NCBI BioSample FTP repository](https://ftp.ncbi.nih.gov/biosample/) as a .gz file, which you can find in the folder [data/samples/ncbi_samples/original](data/samples/ncbi_samples/original). This file contains metadata about 7.8M NCBI samples. To begin, copy the file to the workspace folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file path: data/samples/ncbi_samples/original/2018-03-09-biosample_set.xml.gz\n",
      "Destination path: workspace/data/samples/ncbi_samples/original\n",
      "CPU times: user 331 ms, sys: 1.13 s, total: 1.46 s\n",
      "Wall time: 2.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Copy the .gz file with the NCBI samples used in the evaluation to your workspace folder\n",
    "from shutil import copy\n",
    "import os\n",
    "import scripts.constants as c\n",
    "\n",
    "source_file_path = c.NCBI_SAMPLES_ORIGINAL_FILE_PATH\n",
    "dest_path = os.path.join(c.WORKSPACE_FOLDER, c.NCBI_SAMPLES_ORIGINAL_PATH)\n",
    "print('Source file path: ' + source_file_path)\n",
    "print('Destination path: ' + dest_path)\n",
    "dest_file_name = c.NCBI_SAMPLES_FILE_DEST\n",
    "if not os.path.exists(dest_path):\n",
    "    os.makedirs(dest_path)\n",
    "copy(c.NCBI_SAMPLES_ORIGINAL_FILE_PATH, os.path.join(dest_path, dest_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the NCBI samples file was downloaded on March 9, 2018. Alternatively, if you want to conduct the evaluation with the most recent NCBI samples, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Download the most recent NCBI biosamples to the workspace\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import scripts.util as util\n",
    "import scripts.constants as c\n",
    "\n",
    "url = c.NCBI_DOWNLOAD_URL\n",
    "dest_path = os.path.join(c.WORKSPACE_FOLDER, c.NCBI_SAMPLES_ORIGINAL_PATH)\n",
    "dest_file_name = c.NCBI_SAMPLES_FILE_DEST\n",
    "print('Source URL: ' + url)\n",
    "print('Destination path: ' + dest_path)\n",
    "if not os.path.exists(dest_path):\n",
    "    os.makedirs(dest_path)\n",
    "urllib.request.urlretrieve(url, os.path.join(dest_path, dest_file_name), reporthook=util.log_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s1-b\"></a>1.b. EBI BioSamples\n",
    "We wrote a script ([ebi_biosamples_1_download_split.py](scripts/ebi_biosamples_1_download_split.py)) to download all samples metadata from the [EBI BioSamples database](https://www.ebi.ac.uk/biosamples/) using the [EBI BioSamples API](https://www.ebi.ac.uk/biosamples/help/api.html). We stored the results as a ZIP file [2018-03-09-ebi_samples.zip](data/samples/ebi_samples/original/2018-03-09-ebi_samples.zip) that contains 412 JSON files with metadata for 4.1M samples in total. Extract the file to the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "import scripts.constants as c\n",
    "\n",
    "source_path = c.EBI_SAMPLES_ORIGINAL_FILE_PATH\n",
    "dest_path = os.path.join(c.WORKSPACE_FOLDER, c.EBI_SAMPLES_ORIGINAL_PATH)\n",
    "with zipfile.ZipFile(c.EBI_SAMPLES_ORIGINAL_FILE_PATH, 'r') as zip_obj:\n",
    "    zip_obj.extractall(dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these EBI samples were downloaded on March 9, 2018. If you want to run the evaluation with the most recent EBI samples, you can run [ebi_biosamples_1_download_split.py](scripts/ebi_biosamples_1_download_split.py) again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: download all the EBI samples from the EBI's API\n",
    "%run ./scripts/ebi_biosamples_1_download_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s2\"></a>Step 2: Generation of template instances\n",
    "\n",
    "### <a name=\"s2-1\"></a>2.1. Determine relevant attributes and create CEDAR templates\n",
    "\n",
    "#### <a name=\"s2-1-a\"></a>2.1.a. NCBI BioSample\n",
    "\n",
    "For NCBI BioSample, we created a CEDAR template with all the attributes defined by the [NCBI BioSample Human Package v1.0](https://submit.ncbi.nlm.nih.gov/biosample/template/?package=Human.1.0&action=definition), which are: *biosample_accession, sample_name, sample_title, bioproject_accession, organism, isolate, age, biomaterial_provider, sex, tissue, cell_line, cell_subtype, cell_type, culture_collection, dev_stage, disease, disease_stage, ethnicity, health_state, karyotype, phenotype, population, race, sample_type, treatment, description*.\n",
    "\n",
    "#### <a name=\"s2-1-b\"></a>2.1.b. EBI BioSamples\n",
    "\n",
    "The EBI BioSamples API's output format defines some top-level attributes and makes it possible to add new attributes that describe sample characteristics:\n",
    "```\n",
    "{\n",
    "    \"accession\": \"...\",\n",
    "    \"name\": \"...\",\n",
    "    \"releaseDate\": \"...\",\n",
    "    \"updateDate\": \"...\",\n",
    "    \"characteristics\": { // key-value pairs (e.g., organism, age, sex, organismPart, etc.)\n",
    "    \t...\n",
    "    },\n",
    "    \"organization\": \"...\",\n",
    "    \"contact\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "Based on this format, we defined a metadata template containing 14 fields with general metadata about biological samples and some additional fields that capture specific characteristics of human samples: *accession, name, releaseDate, updateDate, organization, contact, organism, age, sex, organismPart, cellLine, cellType, diseaseState, ethnicity*.\n",
    "\n",
    "We focused our analysis on the subset of fields that meet two key requirements: (1) they are present in both templates and, therefore, can be used to evaluate cross-template recommendations; and (2) they contain categorical values, that is, they represent information about discrete characteristics. We selected 6 fields that met these criteria. These fields are: *sex, organism part, cell line, cell type, disease, and ethnicity*. The names used to refer to these fields in both CEDAR's NCBI BioSample template and CEDAR's EBI BioSamples template are shown in the following table:\n",
    "\n",
    "|Characteristic|NCBI BioSample attribute name|EBI BioSamples attribute name|\n",
    "|---|---|---|\n",
    "|sex|sex|sex|\n",
    "|organism part|tissue|organismPart|\n",
    "|cell line|cell_line|cellLine|\n",
    "|cell type|cell_type|cellType|\n",
    "|disease|disease|diseaseState|\n",
    "|ethnicity|ethnicity|ethnicity|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s2-2\"></a>2.2. Select samples\n",
    "\n",
    "We filtered the samples based on two criteria:\n",
    "* The sample is from \"Homo sapiens\" (organism=Homo sapiens).\n",
    "* The sample has non-empty values for at least 3 of the 6 fields in the previous table.\n",
    "\n",
    "#### <a name=\"s2-2-a\"></a>2.2.a. NCBI BioSample\n",
    "\n",
    "Script used: [ncbi_biosample_1_filter.py](scripts/ncbi_biosample_1_filter.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the NCBI samples\n",
    "%run ./scripts/ncbi_biosample_1_filter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an XML file with 157,653 samples ([biosample_result_filtered.xml](data/samples/ncbi_samples/filtered/biosample_result_filtered.xml)). \n",
    "\n",
    "<font color='blue'>**Shortcut:**</font> copy the precomputed NCBI filtered samples to the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./workspace/data/samples/ncbi_samples/filtered/biosample_result_filtered.xml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shortcut: reuse existing filtered NCBI samples \n",
    "import os\n",
    "from shutil import copyfile\n",
    "import scripts.arm_constants as c\n",
    "\n",
    "src = c.NCBI_FILTER_OUTPUT_FILE_PRECOMPUTED\n",
    "dst = c.NCBI_FILTER_OUTPUT_FILE\n",
    "if not os.path.exists(os.path.dirname(dst)):\n",
    "    os.makedirs(os.path.dirname(dst))\n",
    "copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"s2-2-b\"></a>2.2.b. EBI BioSamples\n",
    "\n",
    "In the case of the EBI samples, we used the script [ebi_biosamples_2_filter.py](scripts/ebi_biosamples_2_filter.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the EBI samples\n",
    "%run ./scripts/ebi_biosamples_2_filter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: 14 JSON files with a total of 135,187 samples, which are available [in this folder](data/samples/ebi_samples/filtered/). \n",
    "\n",
    "<font color='blue'>**Shortcut:**</font> copy the precomputed EBI filtered samples to the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebi_biosamples_filtered_3_20000to29999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_3_20000to29999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_3_20000to29999.json\n",
      "ebi_biosamples_filtered_1_0to9999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_1_0to9999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_1_0to9999.json\n",
      "ebi_biosamples_filtered_2_10000to19999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_2_10000to19999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_2_10000to19999.json\n",
      "ebi_biosamples_filtered_4_30000to39999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_4_30000to39999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_4_30000to39999.json\n",
      "ebi_biosamples_filtered_10_90000to99999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_10_90000to99999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_10_90000to99999.json\n",
      "ebi_biosamples_filtered_5_40000to49999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_5_40000to49999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_5_40000to49999.json\n",
      "ebi_biosamples_filtered_6_50000to59999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_6_50000to59999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_6_50000to59999.json\n",
      "ebi_biosamples_filtered_7_60000to69999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_7_60000to69999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_7_60000to69999.json\n",
      "ebi_biosamples_filtered_9_80000to89999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_9_80000to89999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_9_80000to89999.json\n",
      "ebi_biosamples_filtered_8_70000to79999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_8_70000to79999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_8_70000to79999.json\n",
      "ebi_biosamples_filtered_13_120000to129999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_13_120000to129999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_13_120000to129999.json\n",
      "ebi_biosamples_filtered_11_100000to109999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_11_100000to109999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_11_100000to109999.json\n",
      "ebi_biosamples_filtered_14_130000to135186.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_14_130000to135186.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_14_130000to135186.json\n",
      "ebi_biosamples_filtered_12_110000to119999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_12_110000to119999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_12_110000to119999.json\n"
     ]
    }
   ],
   "source": [
    "# Shortcut: reuse existing filtered EBI samples \n",
    "import os\n",
    "from shutil import copyfile\n",
    "import scripts.arm_constants as c\n",
    "\n",
    "src = c.EBI_FILTER_OUTPUT_FOLDER_PRECOMPUTED\n",
    "dst = c.EBI_FILTER_OUTPUT_FOLDER\n",
    "if not os.path.exists(dst):\n",
    "    os.makedirs(dst)\n",
    "\n",
    "for file_name in os.listdir(src):\n",
    "    print(file_name)\n",
    "    print(os.path.join(src, file_name))\n",
    "    print(os.path.join(dst, file_name))\n",
    "    copyfile(os.path.join(src, file_name), os.path.join(dst, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s2-3\"></a>2.3. Generate CEDAR instances\n",
    "\n",
    "We transformed the NCBI and EBI samples obtained from the previous step to CEDAR template instances conforming to [CEDAR's JSON-based Template Model](https://metadatacenter.org/tools-training/outreach/cedar-template-model).\n",
    "\n",
    "For NCBI samples, we used the script [ncbi_biosample_2_to_cedar_instances.py](scripts/ncbi_biosample_2_to_cedar_instances.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: ./workspace/data/samples/ncbi_samples/filtered/biosample_result_filtered.xml\n",
      "Extracting all samples from file (no. samples: 157653)\n",
      "Randomly picking 135187 samples\n",
      "Generating CEDAR instances...\n",
      "No. instances generated: 10000(7%)\n",
      "No. instances generated: 20000(15%)\n",
      "No. instances generated: 30000(22%)\n",
      "No. instances generated: 40000(30%)\n",
      "No. instances generated: 50000(37%)\n",
      "No. instances generated: 60000(44%)\n",
      "No. instances generated: 70000(52%)\n",
      "No. instances generated: 80000(59%)\n",
      "No. instances generated: 90000(67%)\n",
      "No. instances generated: 100000(74%)\n",
      "No. instances generated: 110000(81%)\n",
      "No. instances generated: 120000(89%)\n",
      "No. instances generated: 130000(96%)\n",
      "Finished\n",
      "CPU times: user 2min 30s, sys: 44.2 s, total: 3min 14s\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate CEDAR instances from NCBI samples\n",
    "%run ./scripts/ncbi_biosample_2_to_cedar_instances.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEDAR's NCBI instances will be saved to [workspace/data/cedar_instances/ncbi_cedar_instances](workspace/data/cedar_instances/ncbi_cedar_instances).\n",
    "\n",
    "For EBI samples, we used the script [ebi_biosamples_3_to_cedar_instances.py](scripts/ebi_biosamples_3_to_cedar_instances.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading EBI biosamples from folder: ./workspace/data/samples/ebi_samples/filtered\n",
      "Total no. samples: 135187\n",
      "Generating CEDAR instances...\n",
      "No. instances generated: 10000(7%)\n",
      "No. instances generated: 20000(15%)\n",
      "No. instances generated: 30000(22%)\n",
      "No. instances generated: 40000(30%)\n",
      "No. instances generated: 50000(37%)\n",
      "No. instances generated: 60000(44%)\n",
      "No. instances generated: 70000(52%)\n",
      "No. instances generated: 80000(59%)\n",
      "No. instances generated: 90000(67%)\n",
      "No. instances generated: 100000(74%)\n",
      "No. instances generated: 110000(81%)\n",
      "No. instances generated: 120000(89%)\n",
      "No. instances generated: 130000(96%)\n",
      "Finished\n",
      "CPU times: user 1min 43s, sys: 46.4 s, total: 2min 30s\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate CEDAR instances from EBI samples\n",
    "%run ./scripts/ebi_biosamples_3_to_cedar_instances.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEDAR's EBI instances will be saved to [workspace/data/cedar_instances/ebi_cedar_instances](workspace/data/cedar_instances/ebi_cedar_instances).\n",
    "\n",
    "All the CEDAR instances using to evaluate the system are available at [data/cedar_instances](data/cedar_instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s3\"></a>Step 3: Semantic annotation\n",
    "\n",
    "We used the [NCBO Annotator](https://bioportal.bioontology.org/annotator) via the [NCBO BioPortal API](http://data.bioontology.org/documentation) to automatically annotate a total of 270,374 template instances (135,187 instances for each template).\n",
    "\n",
    "### <a name=\"s3-1\"></a>3.1. Extraction of unique values from CEDAR instances\n",
    "\n",
    "To avoid making multiple calls to the NCBO Annotator API for the same terms, we first extracted all the unique values in the CEDAR instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting unique values from CEDAR instances...\n",
      "No. instances processed: 10000\n",
      "No. instances processed: 20000\n",
      "No. instances processed: 30000\n",
      "No. instances processed: 40000\n",
      "No. instances processed: 50000\n",
      "No. instances processed: 60000\n",
      "No. instances processed: 70000\n",
      "No. instances processed: 80000\n",
      "No. instances processed: 90000\n",
      "No. instances processed: 100000\n",
      "No. instances processed: 110000\n",
      "No. instances processed: 120000\n",
      "No. instances processed: 130000\n",
      "No. instances processed: 140000\n",
      "No. instances processed: 150000\n",
      "No. instances processed: 160000\n",
      "No. instances processed: 170000\n",
      "No. instances processed: 180000\n",
      "No. instances processed: 190000\n",
      "No. instances processed: 200000\n",
      "No. instances processed: 210000\n",
      "No. instances processed: 220000\n",
      "No. instances processed: 230000\n",
      "No. instances processed: 240000\n",
      "No. instances processed: 250000\n",
      "No. instances processed: 260000\n",
      "No. instances processed: 270000\n",
      "No. unique values extracted: 26556\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extract unique values from NCBI and EBI instances\n",
    "%run ./scripts/cedar_annotator/1_unique_values_extractor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We processed 270,374 instances and obtained 26,556 unique values (see [unique_values.txt](workspace/data/cedar_instances_annotated/unique_values/unique_values.txt)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s3-2\"></a>3.2. Annotation of unique values and generation of mappings\n",
    "\n",
    "We invoked the NCBO Annotator for the unique values obtained from the previous step. Additionally, we took advantage of the output provided by the Annotator API to extract all the different term URIs that map to each term in BioPortal and store all these equivalences into a mappings file. \n",
    "\n",
    "Script used: [2_unique_values_annotator.py](scripts/cedar_annotator/2_unique_values_annotator.py)\n",
    "\n",
    "Note that when running the following cell, you will be asked to enter your BioPortal API key. If you don't have one, follow [these instructions](https://bioportal.bioontology.org/help#Getting_an_API_key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Enter your BioPortal API key\n",
    "bp_api_key = input('Please, enter you BioPortal API key and press Enter:')\n",
    "# Annotate unique values and generate mappings file\n",
    "%run ./scripts/cedar_annotator/2_unique_values_annotator.py --bioportal-api-key $bp_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Shortcut:**</font> if you don't have access to the NCBO Annotator or you don't want to wait for the annotation process to finish, copy the files with the annotated values to your workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/cedar_instances_annotated/unique_values/unique_values_annotated_1.json copied to ./workspace/data/cedar_instances_annotated/unique_values/unique_values_annotated_1.json\n",
      "./data/cedar_instances_annotated/unique_values/unique_values_annotated_2.json copied to ./workspace/data/cedar_instances_annotated/unique_values/unique_values_annotated_2.json\n"
     ]
    }
   ],
   "source": [
    "# Shortcut: reuse previously generated annotations for the unique values\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import scripts.cedar_annotator.annotation_constants as c\n",
    "\n",
    "def my_copy(src, dst):\n",
    "    if not os.path.exists(os.path.dirname(dst)):\n",
    "        os.makedirs(os.path.dirname(dst))\n",
    "    copyfile(src, dst)\n",
    "    print (src + ' copied to ' + dst)\n",
    "\n",
    "src1 = c.VALUES_ANNOTATION_OUTPUT_FILE_PATH_1_PRECOMPUTED\n",
    "dst1 = c.VALUES_ANNOTATION_OUTPUT_FILE_PATH_1\n",
    "src2 = c.VALUES_ANNOTATION_OUTPUT_FILE_PATH_2_PRECOMPUTED\n",
    "dst2 = c.VALUES_ANNOTATION_OUTPUT_FILE_PATH_2\n",
    "\n",
    "my_copy(src1, dst1)\n",
    "my_copy(src2, dst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s3-3\"></a>3.3. Annotation of CEDAR instances\n",
    "\n",
    "This process uses the annotations generated in the previous step to annotate the values of the CEDAR instances without making any additional calls to the BioPortal API. The resulting instances are saved to [workspace/data/cedar_instances_annotated](workspace/data/cedar_instances_annotated).\n",
    "\n",
    "Script: [3_cedar_instances_annotator.py](scripts/cedar_annotator/3_cedar_instances_annotator.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing instances folder: ./workspace/data/cedar_instances/ncbi_cedar_instances/training\n",
      "No. annotated instances: 10000\n",
      "No. annotated instances: 20000\n",
      "No. annotated instances: 30000\n",
      "No. annotated instances: 40000\n",
      "No. annotated instances: 50000\n",
      "No. annotated instances: 60000\n",
      "No. annotated instances: 70000\n",
      "No. annotated instances: 80000\n",
      "No. annotated instances: 90000\n",
      "No. annotated instances: 100000\n",
      "No. annotated instances: 110000\n",
      "\n",
      "No. total values: 379789\n",
      "No. non annotated values: 55518 (15%)\n",
      "Processing instances folder: ./workspace/data/cedar_instances/ncbi_cedar_instances/testing\n",
      "No. annotated instances: 120000\n",
      "No. annotated instances: 130000\n",
      "\n",
      "No. total values: 446822\n",
      "No. non annotated values: 65348 (15%)\n",
      "Processing instances folder: ./workspace/data/cedar_instances/ebi_cedar_instances/training\n",
      "No. annotated instances: 140000\n",
      "No. annotated instances: 150000\n",
      "No. annotated instances: 160000\n",
      "No. annotated instances: 170000\n",
      "No. annotated instances: 180000\n",
      "No. annotated instances: 190000\n",
      "No. annotated instances: 200000\n",
      "No. annotated instances: 210000\n",
      "No. annotated instances: 220000\n",
      "No. annotated instances: 230000\n",
      "No. annotated instances: 240000\n",
      "No. annotated instances: 250000\n",
      "\n",
      "No. total values: 817139\n",
      "No. non annotated values: 117270 (14%)\n",
      "Processing instances folder: ./workspace/data/cedar_instances/ebi_cedar_instances/testing\n",
      "No. annotated instances: 260000\n",
      "No. annotated instances: 270000\n",
      "\n",
      "No. total values: 882498\n",
      "No. non annotated values: 126554 (14%)\n",
      "CPU times: user 3min 48s, sys: 2min 12s, total: 6min 1s\n",
      "Wall time: 2h 14min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate annotated CEDAR instances\n",
    "%run ./scripts/cedar_annotator/3_cedar_instances_annotator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the CEDAR instances using to evaluate the system (both in plain text and annotated) are available at [data/cedar_instances](data/cedar_instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s4\"></a>Step 4: Generation of experimental data sets\n",
    "\n",
    "When we generated the CEDAR instances (step 2.3) and the annotated CEDAR instances (step 3.3), we partitioned the resulting instances for each database (NCBI, EBI) into two datasets, with 85% of the data for training and the remaining 15% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s5\"></a>Step 5: Training\n",
    "\n",
    "We mined association rules from the training sets to discover the hidden relationships between metadata fields. We extracted the rules using a local installation of the CEDAR Workbench. We set up the Value Recommender service to read the instance files from a local folder by updating [its constants file](https://github.com/metadatacenter/cedar-valuerecommender-server/blob/master/cedar-valuerecommender-server-core/src/main/java/org/metadatacenter/intelligentauthoring/valuerecommender/util/Constants.java) as follows:\n",
    "\n",
    "```Java\n",
    "READ_INSTANCES_FROM_CEDAR = false // Read training instances from a local folder\n",
    "```\n",
    "\n",
    "```Java\n",
    "// Apriori configuration:\n",
    "public static final int APRIORI_MAX_NUM_RULES = 1000000;\n",
    "public static int MIN_SUPPORTING_INSTANCES = 5;\n",
    "public static final double MIN_CONFIDENCE = 0.3;\n",
    "public static final double MIN_LIFT = 1.2;\n",
    "public static final double MIN_LEVERAGE = 1.1;\n",
    "public static final double MIN_CONVICTION = 1.1;\n",
    "// Metric types: 0 = Confidence | 1 = Lift | 2 = Leverage | 3 = Conviction\n",
    "public static final int METRIC_TYPE_ID = 0; \n",
    "public static final String SUPPORT_METRIC_NAME = \"Support\";\n",
    "public static final String CONFIDENCE_METRIC_NAME = \"Confidence\";\n",
    "public static final String LIFT_METRIC_NAME = \"Lift\";\n",
    "public static final String LEVERAGE_METRIC_NAME = \"Leverage\";\n",
    "public static final String CONVICTION_METRIC_NAME = \"Conviction\";\n",
    "public static final boolean VERBOSE_MODE = true;\n",
    "```\n",
    "\n",
    "You will have to run the rule extraction process four times, once for each training set. Before each execution, update the variable `CEDAR_INSTANCES_PATH` with the full path of the corresponding training set:\n",
    "* Text-based values:\n",
    "    * To extract the NCBI rules: `.../workspace/data/cedar_instances_annotated/ncbi_cedar_instances/training`\n",
    "    * To extract the EBI rules: `.../workspace/data/cedar_instances_annotated/ebi_cedar_instances/training`\n",
    "* Ontology-based values:\n",
    "    * To extract the NCBI rules: `.../workspace/data/cedar_instances/ncbi_cedar_instances/training`\n",
    "    * To extract the EBI rules: `.../workspace/data/cedar_instances/ebi_cedar_instances/training`\n",
    "\n",
    "Internally, CEDAR's Value Recommender uses a [WEKA's implementation of the Apriori algorithm](https://www.cs.waikato.ac.nz/ml/weka/) with a minimum support of 5 instances and a confidence of 0.3. The final set of rules were indexed using Elasticsearch.\n",
    "\n",
    "Update those constants, compile the `cedar-valuerecommender-server` project and start it locally. You can trigger the rule generation process from the command line using the following curl command:\n",
    "```\n",
    "curl --request POST \\\n",
    "  --url https://valuerecommender.metadatacenter.orgx/command/generate-rules/<TEMPLATE_ID> \\\n",
    "  --header 'authorization: apiKey <CEDAR_ADMIN_API_KEY>' \\\n",
    "  --header 'content-type: application/json' \\\n",
    "  --data '{}'\n",
    "```\n",
    "\n",
    "where `CEDAR_ADMIN_API_KEY` is the API key of the *cedar-admin* user in your local CEDAR system, and `TEMPLATE_ID` is the local identifier of the template that you want to extract rules for, that is, either the identifier of the NCBI BioSample template or the EBI BioSamples template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s5-results\"></a>5.1. Rules generated\n",
    "\n",
    "The following table shows the number of rules produced for each training set and type of metadata. It also provides a link to a .zip file with the produced rules. These files are also available in the [data/rules/](data/rules/) folder.\n",
    "\n",
    "| Training set DB | Type of metadata | No. rules generated | No. rules after filtering | File name       |\n",
    "|-----------------|------------------|---------------------|---------------------------|-----------------|\n",
    "| NCBI            | Text-based       | 52,192              | 30,295                    | [ncbi-text-rules.zip](data/rules/ncbi-text-rules.zip)   |\n",
    "| EBI             | Text-based       | 36,915              | 24,983                    | [ebi-text-rules.zip](data/rules/ebi-text-rules.zip)    |\n",
    "| NCBI            | Ontology-based   | 18,223              | 12,400                    | [ncbi-ont-rules.zip](data/rules/ncbi-ont-rules.zip)    |\n",
    "| EBI             | Ontology-based   | 16,838              | 11,932                    | [ebi-ont-rules.zip](data/rules/ebi-ont-rules.zip)     |\n",
    "\n",
    "We extracted the rules from Elasticsearch using [elasticdump](https://www.npmjs.com/package/elasticdump). \n",
    "\n",
    "Commands used to export the rules and mappings from Elasticsearch to JSON format:\n",
    "\n",
    "- elasticdump --input=http://localhost:9200/cedar-rules --output=./ncbi-text-mappings.json --type=mapping\n",
    "- elasticdump --input=http://localhost:9200/cedar-rules --output=./ncbi-text-data.json --type=data\n",
    "\n",
    "Commands used to import the rules and mappings into Elasticsearch:\n",
    "\n",
    "- elasticdump --input=./ncbi-text-mappings.json --output=http://localhost:9200/cedar-rules  --type=mappings\n",
    "- elasticdump --input=./ncbi-text-data.json --output=http://localhost:9200/cedar-rules --type=data\n",
    "\n",
    "#### Some useful Elasticsearch operations\n",
    "\n",
    "Create an empty rules index in Elasticsearch: run the console command `cedarat rules-regenerateIndex`.\n",
    "\n",
    "Create a backup of the generated rules in ES:\n",
    "```json\n",
    "POST _reindex\n",
    "{\n",
    "  \"source\": {\n",
    "    \"index\": \"cedar-rules\"\n",
    "  },\n",
    "  \"dest\": {\n",
    "    \"index\": \"cedar-rules-backup\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Restore the index backup\n",
    "```json\n",
    "POST _reindex\n",
    "{\n",
    "  \"source\": {\n",
    "    \"index\": \"cedar-rules-backup\"\n",
    "  },\n",
    "  \"dest\": {\n",
    "    \"index\": \"cedar-rules-rules\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s6\"></a>Step 6: Testing\n",
    "\n",
    "In this step, we used the rules generated in the previous step to evaluate the performance of CEDAR's Value Recommender when predicting values from the test sets.\n",
    "\n",
    "Extract most common values: used for baseline recommendations\n",
    "\n",
    "\n",
    "Update:\n",
    "EVALUATION_TRAINING_DB = BIOSAMPLES_DB.NCBI\n",
    "EVALUATION_TESTING_DB = BIOSAMPLES_DB.NCBI\n",
    "EVALUATION_USE_ANNOTATED_VALUES = True\n",
    "\n",
    "Update template identifiers in the constants file:\n",
    "EVALUATION_NCBI_TEMPLATE_ID\n",
    "EVALUATION_EBI_TEMPLATE_ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Enter your CEDAR API key\n",
    "cedar_api_key = input('Please, enter you CEDAR API key and press Enter: ')\n",
    "# Run evaluation\n",
    "%run ./scripts/arm_evaluation_main.py --cedar-api-key $cedar_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s7\"></a>Step 7: Analysis of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Links to the CSV files\n",
    "\n",
    "\n",
    "Link to the R script\n",
    "\n",
    "<img src=\"images/main_experiment/plot_MRR_2019_02_17_14_30_54.png\" alt=\"Mean Reciprocal Rank\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"images/main_experiment/plot_MRR_per_field_2019_02_17_14_31_01.png\" alt=\"Mean Reciprocal Rank per field\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"additional-experiments\"></a>Additional experiments:\n",
    "\n",
    "### <a name=\"additional-experiment-1\"></a>Additional experiment 1: Confidence vs Lift\n",
    "\n",
    "As described in the paper (see Section 3.1.3), the values suggested for the target field are ranked according to a recommendation score that provides an absolute measurement of the goodness of the recommendation. The recommendation score is calculated according to the following expression:\n",
    "\n",
    "$$recommendation\\_score(v') = context\\_matching\\_score(r',C) * conf(r')$$\n",
    "\n",
    "where $v'$ is a value for the target field extracted from the consequent of a selected rule $r'$, $context\\_matching\\_score$ is the function that computes the context-matching score and $conf$ is the function that returns the confidence of a particular rule. Values with the same recommendation score are sorted by support before returning them to the user.\n",
    "\n",
    "In this experiment, we wanted to evaluate the impact of replacing confidence by a different metric known as lift. The lift metric measures the interestingness of importance of a rule and it is widely used in association rule mining. That is, we wanted to evaluate our system when the recommendation score is calculated as:\n",
    "\n",
    "$$recommendation\\_score(v') = context\\_matching\\_score(r',C) * lift(r')$$\n",
    "\n",
    "We also wanted to study the impact of using lift instead of support as a secondary criteria to rank the results that have the same recommendation score.\n",
    "\n",
    "We conducted a new experiment based on metadata from the NCBI BioSample database. We used 10,000 instances as the training set and 1,000 instances as the test set. The results obtained, which are shown in the followin table, confirm that confidence performs better than support in our case. They also show that the second criterion, used to sort the results that have the same recommendation score, influences the final results minimally.\n",
    "\n",
    "| 1st criterion | 2nd criterion | MRR (top 5) |\n",
    "|---------------|---------------|-------------|\n",
    "| confidence    | support       | 0.54        |\n",
    "| confidence    | lift          | 0.53        |\n",
    "| lift          | confidence    | 0.32        |\n",
    "| lift          | support       | 0.32        |\n",
    "\n",
    "\n",
    "### <a name=\"additional-experiment-2\"></a>Additional experiment 2: All fields\n",
    "\n",
    "NCBI template with all the fields (26)\n",
    "Min. confidence: 0.3\n",
    "Min. support: 10 instances\n",
    "Training set size: 5000 instances\n",
    "Testing set size: 500 instances\n",
    "\n",
    "\n",
    "| No. fields | No. rules generated | No. rules after filtering | Rules generation time (seg) | Mean recommendation time (ms)  | MRR   |\n",
    "|------------|---------------------|---------------------------|----------------------------|--------------------------------|-------|\n",
    "| 6          | 775                 | 572                       | 5.10                       | 43.64                          | 0.318 |\n",
    "| 26 (all)   | 233,363             | 30,559                    | 40.81                      | 44.79                          | 0.315 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"links\"></a>Links\n",
    "\n",
    "* [CEDAR Value Recommender documentation](https://github.com/metadatacenter/cedar-docs/wiki/CEDAR-Value-Recommender)\n",
    "* [CEDAR Value Recommender source code](https://github.com/metadatacenter/cedar-valuerecommender-server)\n",
    "* [BioSample demo template](https://cedar.metadatacenter.org/instances/create/https://repo.metadatacenter.org/templates/6d9f4a83-a7ba-42be-a6af-f3cad7b2f7e3?folderId=https:%2F%2Frepo.metadatacenter.org%2Ffolders%2Fdc2ee55c-b891-4576-ba06-bfa3cf11143d)\n",
    "* [Sets of rules generated during the evaluation](#s5-results)\n",
    "* Evaluation results (.csv files) [[text-based]](data/results/text) [[ontology-based]](data/results/annotated)\n",
    "* [CEDAR Workbench User Guide](https://metadatacenter.github.io/cedar-manual/) _(in progress)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"contact\"></a>Contact\n",
    "For any questions about this notebook or about CEDAR's Value Recommender, please contact Marcos Martínez-Romero (marcosmr@stanford.edu)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
