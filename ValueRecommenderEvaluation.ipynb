{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEDAR's Value Recommender - Evaluation\n",
    "\n",
    "This Jupyter notebook describes the steps followed to evaluate [CEDAR's Value Recommender](https://github.com/metadatacenter/cedar-docs/wiki/CEDAR-Value-Recommender), a recommendation system that suggest the most appropriate values for metadata fields. \n",
    "\n",
    "We provide the scripts used to run the evaluation pipeline. All the resulting files will be stored into a local 'workspace' folder.\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "* [Step 1. Datasets download](#s1)\n",
    "    * [1.a. NCBI BioSample](#s1-a)\n",
    "    * [1.b. EBI BioSamples](#s1-b)\n",
    "* [Step 2: Generation of template instances](#s2)\n",
    "    * [2.1. Determine relevant attributes and create CEDAR templates](#s2-1)\n",
    "        * [2.1.a. NCBI BioSample](#s2-1-a)\n",
    "        * [2.1.b. EBI BioSamples](#s2-1-b)\n",
    "    * [2.2. Select samples](#s2-2)\n",
    "        * [2.2.a. NCBI BioSample](#s2-2-a)\n",
    "        * [2.2.b. EBI BioSamples](#s2-2-b)\n",
    "    * [2.3. Generate CEDAR instances](#s2-3)\n",
    "* [Step 3: Semantic annotation](#s3)\n",
    "    * [3.1. Extraction of unique values from CEDAR instances](#s3-1)\n",
    "    * [3.2. Annotation of unique values and generation of mappings](#s3-2)\n",
    "    * [3.3. Annotation of CEDAR instances](#s3-3)\n",
    "* [Step 4: Generation of experimental data sets](#s4)\n",
    "* [Step 5: Training](#s5)\n",
    "    * [5.1. Rules generated](#s5-results)\n",
    "* [Step 6: Testing](#s6)\n",
    "* [Step 7: Analysis of results](#s7)\n",
    "* [Additional experiments](#additional-experiments)\n",
    "    * [Additional experiment 1](#additional-experiment-1)\n",
    "    * [Additional experiment 2](#additional-experiment-2)\n",
    "* [Links](#links)\n",
    "* [Contact](#contact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download\n",
    "Download the data.zip file from https://drive.google.com/a/stanford.edu/file/d/1X8-K1DjRh4FAmRKuGed1XXKsA1iOSl5x/view?usp=sharing and uncompress it in your repository root folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s1\"></a>Step 1: Datasets download\n",
    "### <a name=\"s1-a\"></a>1.a. NCBI BioSample\n",
    "We downloaded the full content of the [NCBI BioSample database](https://www.ncbi.nlm.nih.gov/biosample/) from the [NCBI BioSample FTP repository](https://ftp.ncbi.nih.gov/biosample/) as a .gz file, which you can find in the folder [data/samples/ncbi_samples/original](data/samples/ncbi_samples/original). This file contains metadata about 7.8M NCBI samples. To begin, copy the file to the workspace folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file path: data/samples/ncbi_samples/original/2018-03-09-biosample_set.xml.gz\n",
      "Destination path: workspace/data/samples/ncbi_samples/original\n",
      "CPU times: user 245 ms, sys: 902 ms, total: 1.15 s\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Copy the .gz file with the NCBI samples used in the evaluation to your workspace folder\n",
    "from shutil import copy\n",
    "import os\n",
    "import scripts.constants as c\n",
    "\n",
    "source_file_path = c.NCBI_SAMPLES_ORIGINAL_FILE_PATH\n",
    "dest_path = os.path.join(c.WORKSPACE_FOLDER, c.NCBI_SAMPLES_ORIGINAL_PATH)\n",
    "print('Source file path: ' + source_file_path)\n",
    "print('Destination path: ' + dest_path)\n",
    "dest_file_name = c.NCBI_SAMPLES_FILE_DEST\n",
    "if not os.path.exists(dest_path):\n",
    "    os.makedirs(dest_path)\n",
    "copy(c.NCBI_SAMPLES_ORIGINAL_FILE_PATH, os.path.join(dest_path, dest_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the NCBI samples file was downloaded on March 9, 2018. Alternatively, if you want to conduct the evaluation with the most recent NCBI samples, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Download the most recent NCBI biosamples to the workspace\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import scripts.util as util\n",
    "import scripts.constants as c\n",
    "\n",
    "url = c.NCBI_DOWNLOAD_URL\n",
    "dest_path = os.path.join(c.WORKSPACE_FOLDER, c.NCBI_SAMPLES_ORIGINAL_PATH)\n",
    "dest_file_name = c.NCBI_SAMPLES_FILE_DEST\n",
    "print('Source URL: ' + url)\n",
    "print('Destination path: ' + dest_path)\n",
    "if not os.path.exists(dest_path):\n",
    "    os.makedirs(dest_path)\n",
    "urllib.request.urlretrieve(url, os.path.join(dest_path, dest_file_name), reporthook=util.log_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s1-b\"></a>1.b. EBI BioSamples\n",
    "We wrote a script ([ebi_biosamples_1_download_split.py](scripts/ebi_biosamples_1_download_split.py)) to download all samples metadata from the [EBI BioSamples database](https://www.ebi.ac.uk/biosamples/) using the [EBI BioSamples API](https://www.ebi.ac.uk/biosamples/help/api.html). We stored the results as a ZIP file [2018-03-09-ebi_samples.zip](data/samples/ebi_samples/original/2018-03-09-ebi_samples.zip) that contains 412 JSON files with metadata for 4.1M samples in total. Extract the file to the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "import scripts.constants as c\n",
    "\n",
    "source_path = c.EBI_SAMPLES_ORIGINAL_FILE_PATH\n",
    "dest_path = os.path.join(c.WORKSPACE_FOLDER, c.EBI_SAMPLES_ORIGINAL_PATH)\n",
    "with zipfile.ZipFile(c.EBI_SAMPLES_ORIGINAL_FILE_PATH, 'r') as zip_obj:\n",
    "    zip_obj.extractall(dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these EBI samples were downloaded on March 9, 2018. If you want to run the evaluation with the most recent EBI samples, you can run [ebi_biosamples_1_download_split.py](scripts/ebi_biosamples_1_download_split.py) again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: download all the EBI samples from the EBI's API\n",
    "%run ./scripts/ebi_biosamples_1_download_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s2\"></a>Step 2: Generation of template instances\n",
    "\n",
    "### <a name=\"s2-1\"></a>2.1. Determine relevant attributes and create CEDAR templates\n",
    "\n",
    "#### <a name=\"s2-1-a\"></a>2.1.a. NCBI BioSample\n",
    "\n",
    "For NCBI BioSample, we created a CEDAR template with all the attributes defined by the [NCBI BioSample Human Package v1.0](https://submit.ncbi.nlm.nih.gov/biosample/template/?package=Human.1.0&action=definition), which are: *biosample_accession, sample_name, sample_title, bioproject_accession, organism, isolate, age, biomaterial_provider, sex, tissue, cell_line, cell_subtype, cell_type, culture_collection, dev_stage, disease, disease_stage, ethnicity, health_state, karyotype, phenotype, population, race, sample_type, treatment, description*.\n",
    "\n",
    "#### <a name=\"s2-1-b\"></a>2.1.b. EBI BioSamples\n",
    "\n",
    "The EBI BioSamples API's output format defines some top-level attributes and makes it possible to add new attributes that describe sample characteristics:\n",
    "```\n",
    "{\n",
    "    \"accession\": \"...\",\n",
    "    \"name\": \"...\",\n",
    "    \"releaseDate\": \"...\",\n",
    "    \"updateDate\": \"...\",\n",
    "    \"characteristics\": { // key-value pairs (e.g., organism, age, sex, organismPart, etc.)\n",
    "    \t...\n",
    "    },\n",
    "    \"organization\": \"...\",\n",
    "    \"contact\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "Based on this format, we defined a metadata template containing 14 fields with general metadata about biological samples and some additional fields that capture specific characteristics of human samples: *accession, name, releaseDate, updateDate, organization, contact, organism, age, sex, organismPart, cellLine, cellType, diseaseState, ethnicity*.\n",
    "\n",
    "We focused our analysis on the subset of fields that meet two key requirements: (1) they are present in both templates and, therefore, can be used to evaluate cross-template recommendations; and (2) they contain categorical values, that is, they represent information about discrete characteristics. We selected 6 fields that met these criteria. These fields are: *sex, organism part, cell line, cell type, disease, and ethnicity*. The names used to refer to these fields in both CEDAR's NCBI BioSample template and CEDAR's EBI BioSamples template are shown in the following table:\n",
    "\n",
    "|Characteristic|NCBI BioSample attribute name|EBI BioSamples attribute name|\n",
    "|---|---|---|\n",
    "|sex|sex|sex|\n",
    "|organism part|tissue|organismPart|\n",
    "|cell line|cell_line|cellLine|\n",
    "|cell type|cell_type|cellType|\n",
    "|disease|disease|diseaseState|\n",
    "|ethnicity|ethnicity|ethnicity|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s2-2\"></a>2.2. Select samples\n",
    "\n",
    "We filtered the samples based on two criteria:\n",
    "* The sample is from \"Homo sapiens\" (organism=Homo sapiens).\n",
    "* The sample has non-empty values for at least 3 of the 6 fields in the previous table.\n",
    "\n",
    "#### <a name=\"s2-2-a\"></a>2.2.a. NCBI BioSample\n",
    "\n",
    "Script used: [ncbi_biosample_1_filter.py](scripts/ncbi_biosample_1_filter.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: ./workspace/data/samples/ncbi_samples/original/biosample_set.xml.gz\n",
      "Processing NCBI samples...\n",
      "Processed samples: 5000\n",
      "Selected samples: 0\n",
      "Processed samples: 10000\n",
      "Selected samples: 2\n",
      "Processed samples: 15000\n",
      "Selected samples: 59\n",
      "Processed samples: 20000\n",
      "Selected samples: 67\n",
      "Processed samples: 25000\n",
      "Selected samples: 67\n",
      "Processed samples: 30000\n",
      "Selected samples: 67\n",
      "Processed samples: 35000\n",
      "Selected samples: 67\n",
      "Processed samples: 40000\n",
      "Selected samples: 67\n",
      "Processed samples: 45000\n",
      "Selected samples: 67\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Development/git_repos/CEDAR/cedar-experiments/value-recommender-evaluation/scripts/ncbi_biosample_1_filter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Development/git_repos/CEDAR/cedar-experiments/value-recommender-evaluation/scripts/ncbi_biosample_1_filter.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'START_ELEMENT'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagName\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'BioSample'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mnode_xml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoxml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mprocessed_samples_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_samples_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/dom/pulldom.py\u001b[0m in \u001b[0;36mexpandNode\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEND_ELEMENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/dom/pulldom.py\u001b[0m in \u001b[0;36mgetEvent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpulldom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstEvent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpulldom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstEvent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpulldom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstEvent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/sax/expatreader.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data, isFinal)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# document. When feeding chunks, they are not normally final -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;31m# except when invoked from close.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misFinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAXParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mErrorString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/tmp/python-20180822-57632-mw5sr/Python-3.7.0/Modules/pyexpat.c\u001b[0m in \u001b[0;36mCharacterData\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/dom/pulldom.py\u001b[0m in \u001b[0;36mcharacters\u001b[0;34m(self, chars)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastEvent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastEvent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mcharacters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTextNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastEvent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHARACTERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Filter the NCBI samples\n",
    "%run ./scripts/ncbi_biosample_1_filter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an XML file with 157,653 samples ([biosample_result_filtered.xml](data/samples/ncbi_samples/filtered/biosample_result_filtered.xml)). \n",
    "\n",
    "<font color='blue'>**Shortcut:**</font> copy the precomputed NCBI filtered samples to the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./workspace/data/samples/ncbi_samples/filtered/biosample_result_filtered.xml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shortcut: reuse existing filtered NCBI samples \n",
    "import os\n",
    "from shutil import copyfile\n",
    "import scripts.arm_constants as c\n",
    "\n",
    "src = c.NCBI_FILTER_OUTPUT_FILE_PRECOMPUTED\n",
    "dst = c.NCBI_FILTER_OUTPUT_FILE\n",
    "if not os.path.exists(os.path.dirname(dst)):\n",
    "    os.makedirs(os.path.dirname(dst))\n",
    "copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"s2-2-b\"></a>2.2.b. EBI BioSamples\n",
    "\n",
    "In the case of the EBI samples, we used the script [ebi_biosamples_2_filter.py](scripts/ebi_biosamples_2_filter.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 00001 ebi_biosamples_1to10000.json\n",
      "Accumulated selected samples: 1029\n",
      "Processing file: 00002 ebi_biosamples_10001to20000.json\n",
      "Accumulated selected samples: 1897\n",
      "Processing file: 00003 ebi_biosamples_20001to30000.json\n",
      "Accumulated selected samples: 2555\n",
      "Processing file: 00004 ebi_biosamples_30001to40000.json\n",
      "Accumulated selected samples: 3434\n",
      "Processing file: 00005 ebi_biosamples_40001to50000.json\n",
      "Accumulated selected samples: 4372\n",
      "Processing file: 00006 ebi_biosamples_50001to60000.json\n",
      "Accumulated selected samples: 5147\n",
      "Processing file: 00007 ebi_biosamples_60001to70000.json\n",
      "Accumulated selected samples: 5611\n",
      "Processing file: 00008 ebi_biosamples_70001to80000.json\n",
      "Accumulated selected samples: 5810\n",
      "Processing file: 00009 ebi_biosamples_80001to90000.json\n",
      "Accumulated selected samples: 6336\n",
      "Processing file: 00010 ebi_biosamples_90001to100000.json\n",
      "Accumulated selected samples: 6567\n",
      "Processing file: 00011 ebi_biosamples_100001to110000.json\n",
      "Accumulated selected samples: 6746\n",
      "Processing file: 00012 ebi_biosamples_110001to120000.json\n",
      "Accumulated selected samples: 6972\n",
      "Processing file: 00013 ebi_biosamples_120001to130000.json\n",
      "Accumulated selected samples: 7037\n",
      "Processing file: 00014 ebi_biosamples_130001to140000.json\n",
      "Accumulated selected samples: 7553\n",
      "Processing file: 00015 ebi_biosamples_140001to150000.json\n",
      "Accumulated selected samples: 8002\n",
      "Processing file: 00016 ebi_biosamples_150001to160000.json\n",
      "Accumulated selected samples: 8384\n",
      "Processing file: 00017 ebi_biosamples_160001to170000.json\n",
      "Accumulated selected samples: 8913\n",
      "Processing file: 00018 ebi_biosamples_170001to180000.json\n",
      "Accumulated selected samples: 9512\n",
      "Processing file: 00019 ebi_biosamples_180001to190000.json\n",
      "Accumulated selected samples: 9870\n",
      "Processing file: 00020 ebi_biosamples_190001to200000.json\n",
      "Accumulated selected samples: 10307\n",
      "Processing file: 00021 ebi_biosamples_200001to210000.json\n",
      "Accumulated selected samples: 10762\n",
      "Processing file: 00022 ebi_biosamples_210001to220000.json\n",
      "Accumulated selected samples: 11127\n",
      "Processing file: 00023 ebi_biosamples_220001to230000.json\n",
      "Accumulated selected samples: 11502\n",
      "Processing file: 00024 ebi_biosamples_230001to240000.json\n",
      "Accumulated selected samples: 11902\n",
      "Processing file: 00025 ebi_biosamples_240001to250000.json\n",
      "Accumulated selected samples: 12173\n",
      "Processing file: 00026 ebi_biosamples_250001to260000.json\n",
      "Accumulated selected samples: 12641\n",
      "Processing file: 00027 ebi_biosamples_260001to270000.json\n",
      "Accumulated selected samples: 13319\n",
      "Processing file: 00028 ebi_biosamples_270001to280000.json\n",
      "Accumulated selected samples: 13857\n",
      "Processing file: 00029 ebi_biosamples_280001to290000.json\n",
      "Accumulated selected samples: 14539\n",
      "Processing file: 00030 ebi_biosamples_290001to300000.json\n",
      "Accumulated selected samples: 14791\n",
      "Processing file: 00031 ebi_biosamples_300001to310000.json\n",
      "Accumulated selected samples: 15198\n",
      "Processing file: 00032 ebi_biosamples_310001to320000.json\n",
      "Accumulated selected samples: 15342\n",
      "Processing file: 00033 ebi_biosamples_320001to330000.json\n",
      "Accumulated selected samples: 15775\n",
      "Processing file: 00034 ebi_biosamples_330001to340000.json\n",
      "Accumulated selected samples: 16290\n",
      "Processing file: 00035 ebi_biosamples_340001to350000.json\n",
      "Accumulated selected samples: 16570\n",
      "Processing file: 00036 ebi_biosamples_350001to360000.json\n",
      "Accumulated selected samples: 16953\n",
      "Processing file: 00037 ebi_biosamples_360001to370000.json\n",
      "Accumulated selected samples: 17476\n",
      "Processing file: 00038 ebi_biosamples_370001to380000.json\n",
      "Accumulated selected samples: 17978\n",
      "Processing file: 00039 ebi_biosamples_380001to390000.json\n",
      "Accumulated selected samples: 18482\n",
      "Processing file: 00040 ebi_biosamples_390001to400000.json\n",
      "Accumulated selected samples: 18923\n",
      "Processing file: 00041 ebi_biosamples_400001to410000.json\n",
      "Accumulated selected samples: 19119\n",
      "Processing file: 00042 ebi_biosamples_410001to420000.json\n",
      "Accumulated selected samples: 19260\n",
      "Processing file: 00043 ebi_biosamples_420001to430000.json\n",
      "Accumulated selected samples: 19466\n",
      "Processing file: 00044 ebi_biosamples_430001to440000.json\n",
      "Accumulated selected samples: 19805\n",
      "Processing file: 00045 ebi_biosamples_440001to450000.json\n",
      "Accumulated selected samples: 20265\n",
      "Processing file: 00046 ebi_biosamples_450001to460000.json\n",
      "Accumulated selected samples: 20723\n",
      "Processing file: 00047 ebi_biosamples_460001to470000.json\n",
      "Accumulated selected samples: 21048\n",
      "Processing file: 00048 ebi_biosamples_470001to480000.json\n",
      "Accumulated selected samples: 21320\n",
      "Processing file: 00049 ebi_biosamples_480001to490000.json\n",
      "Accumulated selected samples: 21768\n",
      "Processing file: 00050 ebi_biosamples_490001to500000.json\n",
      "Accumulated selected samples: 22463\n",
      "Processing file: 00051 ebi_biosamples_500001to510000.json\n",
      "Accumulated selected samples: 22699\n",
      "Processing file: 00052 ebi_biosamples_510001to520000.json\n",
      "Accumulated selected samples: 22877\n",
      "Processing file: 00053 ebi_biosamples_520001to530000.json\n",
      "Accumulated selected samples: 23161\n",
      "Processing file: 00054 ebi_biosamples_530001to540000.json\n",
      "Accumulated selected samples: 23393\n",
      "Processing file: 00055 ebi_biosamples_540001to550000.json\n",
      "Accumulated selected samples: 23697\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Development/git_repos/CEDAR/cedar-experiments/value-recommender-evaluation/scripts/ebi_biosamples_2_filter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Development/git_repos/CEDAR/cedar-experiments/value-recommender-evaluation/scripts/ebi_biosamples_2_filter.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mselected_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_remove_filters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;31m# export to files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exporting selected samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/git_repos/CEDAR/cedar-experiments/value-recommender-evaluation/scripts/ebi_biosamples_2_filter.py\u001b[0m in \u001b[0;36mapply_remove_filters\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\".json\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# basic check to be sure that we are processing the right files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0msamples_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mtotal_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_processed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing file: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Filter the EBI samples\n",
    "%run ./scripts/ebi_biosamples_2_filter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: 14 JSON files with a total of 135,187 samples, which are available [in this folder](data/samples/ebi_samples/filtered/). \n",
    "\n",
    "<font color='blue'>**Shortcut:**</font> copy the precomputed EBI filtered samples to the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebi_biosamples_filtered_3_20000to29999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_3_20000to29999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_3_20000to29999.json\n",
      "ebi_biosamples_filtered_1_0to9999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_1_0to9999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_1_0to9999.json\n",
      "ebi_biosamples_filtered_2_10000to19999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_2_10000to19999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_2_10000to19999.json\n",
      "ebi_biosamples_filtered_4_30000to39999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_4_30000to39999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_4_30000to39999.json\n",
      "ebi_biosamples_filtered_10_90000to99999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_10_90000to99999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_10_90000to99999.json\n",
      "ebi_biosamples_filtered_5_40000to49999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_5_40000to49999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_5_40000to49999.json\n",
      "ebi_biosamples_filtered_6_50000to59999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_6_50000to59999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_6_50000to59999.json\n",
      "ebi_biosamples_filtered_7_60000to69999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_7_60000to69999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_7_60000to69999.json\n",
      "ebi_biosamples_filtered_9_80000to89999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_9_80000to89999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_9_80000to89999.json\n",
      "ebi_biosamples_filtered_8_70000to79999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_8_70000to79999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_8_70000to79999.json\n",
      "ebi_biosamples_filtered_13_120000to129999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_13_120000to129999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_13_120000to129999.json\n",
      "ebi_biosamples_filtered_11_100000to109999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_11_100000to109999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_11_100000to109999.json\n",
      "ebi_biosamples_filtered_14_130000to135186.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_14_130000to135186.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_14_130000to135186.json\n",
      "ebi_biosamples_filtered_12_110000to119999.json\n",
      "./data/samples/ebi_samples/filtered/ebi_biosamples_filtered_12_110000to119999.json\n",
      "./workspace/data/samples/ebi_samples/filtered/ebi_biosamples_filtered_12_110000to119999.json\n"
     ]
    }
   ],
   "source": [
    "# Shortcut: reuse existing filtered EBI samples \n",
    "import os\n",
    "from shutil import copyfile\n",
    "import scripts.arm_constants as c\n",
    "\n",
    "src = c.EBI_FILTER_OUTPUT_FOLDER_PRECOMPUTED\n",
    "dst = c.EBI_FILTER_OUTPUT_FOLDER\n",
    "if not os.path.exists(dst):\n",
    "    os.makedirs(dst)\n",
    "\n",
    "for file_name in os.listdir(src):\n",
    "    print(file_name)\n",
    "    print(os.path.join(src, file_name))\n",
    "    print(os.path.join(dst, file_name))\n",
    "    copyfile(os.path.join(src, file_name), os.path.join(dst, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s2-3\"></a>2.3. Generate CEDAR instances\n",
    "\n",
    "We transformed the NCBI and EBI samples obtained from the previous step to CEDAR template instances conforming to [CEDAR's JSON-based Template Model](https://metadatacenter.org/tools-training/outreach/cedar-template-model).\n",
    "\n",
    "For NCBI samples, we used the script [ncbi_biosample_2_to_cedar_instances.py](scripts/ncbi_biosample_2_to_cedar_instances.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: ./workspace/data/samples/ncbi_samples/filtered/biosample_result_filtered.xml\n",
      "Extracting all samples from file (no. samples: 157653)\n",
      "Randomly picking 135187 samples\n",
      "Generating CEDAR instances...\n",
      "No. instances generated: 10000(7%)\n",
      "No. instances generated: 20000(15%)\n",
      "No. instances generated: 30000(22%)\n",
      "No. instances generated: 40000(30%)\n",
      "No. instances generated: 50000(37%)\n",
      "No. instances generated: 60000(44%)\n",
      "No. instances generated: 70000(52%)\n",
      "No. instances generated: 80000(59%)\n",
      "No. instances generated: 90000(67%)\n",
      "No. instances generated: 100000(74%)\n",
      "No. instances generated: 110000(81%)\n",
      "No. instances generated: 120000(89%)\n",
      "No. instances generated: 130000(96%)\n",
      "Finished\n",
      "CPU times: user 2min 30s, sys: 44.2 s, total: 3min 14s\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate CEDAR instances from NCBI samples\n",
    "%run ./scripts/ncbi_biosample_2_to_cedar_instances.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEDAR's NCBI instances will be saved to [workspace/data/cedar_instances/ncbi_cedar_instances](workspace/data/cedar_instances/ncbi_cedar_instances).\n",
    "\n",
    "For EBI samples, we used the script [ebi_biosamples_3_to_cedar_instances.py](scripts/ebi_biosamples_3_to_cedar_instances.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading EBI biosamples from folder: ./workspace/data/samples/ebi_samples/filtered\n",
      "Total no. samples: 135187\n",
      "Generating CEDAR instances...\n",
      "No. instances generated: 10000(7%)\n",
      "No. instances generated: 20000(15%)\n",
      "No. instances generated: 30000(22%)\n",
      "No. instances generated: 40000(30%)\n",
      "No. instances generated: 50000(37%)\n",
      "No. instances generated: 60000(44%)\n",
      "No. instances generated: 70000(52%)\n",
      "No. instances generated: 80000(59%)\n",
      "No. instances generated: 90000(67%)\n",
      "No. instances generated: 100000(74%)\n",
      "No. instances generated: 110000(81%)\n",
      "No. instances generated: 120000(89%)\n",
      "No. instances generated: 130000(96%)\n",
      "Finished\n",
      "CPU times: user 1min 43s, sys: 46.4 s, total: 2min 30s\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate CEDAR instances from EBI samples\n",
    "%run ./scripts/ebi_biosamples_3_to_cedar_instances.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEDAR's EBI instances will be saved to [workspace/data/cedar_instances/ebi_cedar_instances](workspace/data/cedar_instances/ebi_cedar_instances).\n",
    "\n",
    "All the CEDAR instances using to evaluate the system are available at [data/cedar_instances](data/cedar_instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s3\"></a>Step 3: Semantic annotation\n",
    "\n",
    "We used the [NCBO Annotator](https://bioportal.bioontology.org/annotator) via the [NCBO BioPortal API](http://data.bioontology.org/documentation) to automatically annotate a total of 270,374 template instances (135,187 instances for each template).\n",
    "\n",
    "### <a name=\"s3-1\"></a>3.1. Extraction of unique values from CEDAR instances\n",
    "\n",
    "To avoid making multiple calls to the NCBO Annotator API for the same terms, we first extracted all the unique values in the CEDAR instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting unique values from CEDAR instances...\n",
      "No. instances processed: 10000\n",
      "No. instances processed: 20000\n",
      "No. instances processed: 30000\n",
      "No. instances processed: 40000\n",
      "No. instances processed: 50000\n",
      "No. instances processed: 60000\n",
      "No. instances processed: 70000\n",
      "No. instances processed: 80000\n",
      "No. instances processed: 90000\n",
      "No. instances processed: 100000\n",
      "No. instances processed: 110000\n",
      "No. instances processed: 120000\n",
      "No. instances processed: 130000\n",
      "No. instances processed: 140000\n",
      "No. instances processed: 150000\n",
      "No. instances processed: 160000\n",
      "No. instances processed: 170000\n",
      "No. instances processed: 180000\n",
      "No. instances processed: 190000\n",
      "No. instances processed: 200000\n",
      "No. instances processed: 210000\n",
      "No. instances processed: 220000\n",
      "No. instances processed: 230000\n",
      "No. instances processed: 240000\n",
      "No. instances processed: 250000\n",
      "No. instances processed: 260000\n",
      "No. instances processed: 270000\n",
      "No. unique values extracted: 26556\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extract unique values from NCBI and EBI instances\n",
    "%run ./scripts/cedar_annotator/1_unique_values_extractor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We processed 270,374 instances and obtained 26,556 unique values (see [unique_values.txt](workspace/data/cedar_instances_annotated/unique_values/unique_values.txt)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s3-2\"></a>3.2. Annotation of unique values and generation of mappings\n",
    "\n",
    "We invoked the NCBO Annotator for the unique values obtained from the previous step. Additionally, we took advantage of the output provided by the Annotator API to extract all the different term URIs that map to each term in BioPortal and store all these equivalences into a mappings file. \n",
    "\n",
    "Script used: [2_unique_values_annotator.py](scripts/cedar_annotator/2_unique_values_annotator.py)\n",
    "\n",
    "Note that when running the following cell, you will be asked to enter your BioPortal API key. If you don't have one, follow [these instructions](https://bioportal.bioontology.org/help#Getting_an_API_key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Enter your BioPortal API key\n",
    "bp_api_key = input('Please, enter you BioPortal API key and press Enter:')\n",
    "# Annotate unique values and generate mappings file\n",
    "%run ./scripts/cedar_annotator/2_unique_values_annotator.py --bioportal-api-key $bp_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Shortcut:**</font> if you don't have access to the NCBO Annotator or you don't want to wait for the annotation process to finish, copy the files with the annotated values to your workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/cedar_instances_annotated/unique_values/unique_values_annotated_1.json copied to ./workspace/data/cedar_instances_annotated/unique_values/unique_values_annotated_1.json\n",
      "./data/cedar_instances_annotated/unique_values/unique_values_annotated_2.json copied to ./workspace/data/cedar_instances_annotated/unique_values/unique_values_annotated_2.json\n"
     ]
    }
   ],
   "source": [
    "# Shortcut: reuse previously generated annotations for the unique values\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import scripts.cedar_annotator.annotation_constants as c\n",
    "\n",
    "def my_copy(src, dst):\n",
    "    if not os.path.exists(os.path.dirname(dst)):\n",
    "        os.makedirs(os.path.dirname(dst))\n",
    "    copyfile(src, dst)\n",
    "    print (src + ' copied to ' + dst)\n",
    "\n",
    "src1 = c.VALUES_ANNOTATION_OUTPUT_FILE_PATH_1_PRECOMPUTED\n",
    "dst1 = c.VALUES_ANNOTATION_OUTPUT_FILE_PATH_1\n",
    "src2 = c.VALUES_ANNOTATION_OUTPUT_FILE_PATH_2_PRECOMPUTED\n",
    "dst2 = c.VALUES_ANNOTATION_OUTPUT_FILE_PATH_2\n",
    "\n",
    "my_copy(src1, dst1)\n",
    "my_copy(src2, dst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s3-3\"></a>3.3. Annotation of CEDAR instances\n",
    "\n",
    "This process uses the annotations generated in the previous step to annotate the values of the CEDAR instances without making any additional calls to the BioPortal API. The resulting instances are saved to [workspace/data/cedar_instances_annotated](workspace/data/cedar_instances_annotated).\n",
    "\n",
    "Script: [3_cedar_instances_annotator.py](scripts/cedar_annotator/3_cedar_instances_annotator.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing instances folder: ./workspace/data/cedar_instances/ncbi_cedar_instances/training\n",
      "No. annotated instances: 10000\n",
      "No. annotated instances: 20000\n",
      "No. annotated instances: 30000\n",
      "No. annotated instances: 40000\n",
      "No. annotated instances: 50000\n",
      "No. annotated instances: 60000\n",
      "No. annotated instances: 70000\n",
      "No. annotated instances: 80000\n",
      "No. annotated instances: 90000\n",
      "No. annotated instances: 100000\n",
      "No. annotated instances: 110000\n",
      "\n",
      "No. total values: 379789\n",
      "No. non annotated values: 55518 (15%)\n",
      "Processing instances folder: ./workspace/data/cedar_instances/ncbi_cedar_instances/testing\n",
      "No. annotated instances: 120000\n",
      "No. annotated instances: 130000\n",
      "\n",
      "No. total values: 446822\n",
      "No. non annotated values: 65348 (15%)\n",
      "Processing instances folder: ./workspace/data/cedar_instances/ebi_cedar_instances/training\n",
      "No. annotated instances: 140000\n",
      "No. annotated instances: 150000\n",
      "No. annotated instances: 160000\n",
      "No. annotated instances: 170000\n",
      "No. annotated instances: 180000\n",
      "No. annotated instances: 190000\n",
      "No. annotated instances: 200000\n",
      "No. annotated instances: 210000\n",
      "No. annotated instances: 220000\n",
      "No. annotated instances: 230000\n",
      "No. annotated instances: 240000\n",
      "No. annotated instances: 250000\n",
      "\n",
      "No. total values: 817139\n",
      "No. non annotated values: 117270 (14%)\n",
      "Processing instances folder: ./workspace/data/cedar_instances/ebi_cedar_instances/testing\n",
      "No. annotated instances: 260000\n",
      "No. annotated instances: 270000\n",
      "\n",
      "No. total values: 882498\n",
      "No. non annotated values: 126554 (14%)\n",
      "CPU times: user 3min 48s, sys: 2min 12s, total: 6min 1s\n",
      "Wall time: 2h 14min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate annotated CEDAR instances\n",
    "%run ./scripts/cedar_annotator/3_cedar_instances_annotator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the CEDAR instances using to evaluate the system (both in plain text and annotated) are available at [data/cedar_instances](data/cedar_instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s4\"></a>Step 4: Generation of experimental data sets\n",
    "\n",
    "When we generated the CEDAR instances (step 2.3) and the annotated CEDAR instances (step 3.3), we partitioned the resulting instances for each database (NCBI, EBI) into two datasets, with 85% of the data for training and the remaining 15% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s5\"></a>Step 5: Training\n",
    "\n",
    "We mined association rules from the training sets to discover the hidden relationships between metadata fields. We extracted the rules using a local installation of the CEDAR Workbench. We set up the Value Recommender service to read the instance files from a local folder by updating [its constants file](https://github.com/metadatacenter/cedar-valuerecommender-server/blob/master/cedar-valuerecommender-server-core/src/main/java/org/metadatacenter/intelligentauthoring/valuerecommender/util/Constants.java) as follows:\n",
    "\n",
    "```Java\n",
    "READ_INSTANCES_FROM_CEDAR = false // Read training instances from a local folder\n",
    "```\n",
    "\n",
    "```Java\n",
    "// Apriori configuration:\n",
    "public static final int APRIORI_MAX_NUM_RULES = 1000000;\n",
    "public static int MIN_SUPPORTING_INSTANCES = 5;\n",
    "public static final double MIN_CONFIDENCE = 0.3;\n",
    "public static final double MIN_LIFT = 1.2;\n",
    "public static final double MIN_LEVERAGE = 1.1;\n",
    "public static final double MIN_CONVICTION = 1.1;\n",
    "// Metric types: 0 = Confidence | 1 = Lift | 2 = Leverage | 3 = Conviction\n",
    "public static final int METRIC_TYPE_ID = 0; \n",
    "public static final String SUPPORT_METRIC_NAME = \"Support\";\n",
    "public static final String CONFIDENCE_METRIC_NAME = \"Confidence\";\n",
    "public static final String LIFT_METRIC_NAME = \"Lift\";\n",
    "public static final String LEVERAGE_METRIC_NAME = \"Leverage\";\n",
    "public static final String CONVICTION_METRIC_NAME = \"Conviction\";\n",
    "public static final boolean VERBOSE_MODE = true;\n",
    "```\n",
    "\n",
    "You will have to run the rule extraction process four times, once for each training set. Before each execution, update the variable `CEDAR_INSTANCES_PATH` with the full path of the corresponding training set:\n",
    "* Text-based values:\n",
    "    * To extract the NCBI rules: `.../workspace/data/cedar_instances_annotated/ncbi_cedar_instances/training`\n",
    "    * To extract the EBI rules: `.../workspace/data/cedar_instances_annotated/ebi_cedar_instances/training`\n",
    "* Ontology-based values:\n",
    "    * To extract the NCBI rules: `.../workspace/data/cedar_instances/ncbi_cedar_instances/training`\n",
    "    * To extract the EBI rules: `.../workspace/data/cedar_instances/ebi_cedar_instances/training`\n",
    "\n",
    "Internally, CEDAR's Value Recommender uses a [WEKA's implementation of the Apriori algorithm](https://www.cs.waikato.ac.nz/ml/weka/) with a minimum support of 5 instances and a confidence of 0.3. The final set of rules were indexed using Elasticsearch.\n",
    "\n",
    "Update those constants, compile the `cedar-valuerecommender-server` project and start it locally. You can trigger the rule generation process from the command line using the following curl command:\n",
    "```\n",
    "curl --request POST \\\n",
    "  --url https://valuerecommender.metadatacenter.orgx/command/generate-rules/<TEMPLATE_ID> \\\n",
    "  --header 'authorization: apiKey <CEDAR_ADMIN_API_KEY>' \\\n",
    "  --header 'content-type: application/json' \\\n",
    "  --data '{}'\n",
    "```\n",
    "\n",
    "where `CEDAR_ADMIN_API_KEY` is the API key of the *cedar-admin* user in your local CEDAR system, and `TEMPLATE_ID` is the local identifier of the template that you want to extract rules for, that is, either the identifier of the NCBI BioSample template or the EBI BioSamples template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"s5-results\"></a>5.1. Rules generated\n",
    "\n",
    "The following table shows the number of rules produced for each training set and type of metadata. It also provides a link to a .zip file with the produced rules. These files are also available in the [data/rules/](data/rules/) folder.\n",
    "\n",
    "| Training set DB | Type of metadata | No. rules generated | No. rules after filtering | File name       |\n",
    "|-----------------|------------------|---------------------|---------------------------|-----------------|\n",
    "| NCBI            | Text-based       | 52,192              | 30,295                    | [ncbi-text-rules.zip](data/rules/ncbi-text-rules.zip)   |\n",
    "| EBI             | Text-based       | 36,915              | 24,983                    | [ebi-text-rules.zip](data/rules/ebi-text-rules.zip)    |\n",
    "| NCBI            | Ontology-based   | 18,223              | 12,400                    | [ncbi-ont-rules.zip](data/rules/ncbi-ont-rules.zip)    |\n",
    "| EBI             | Ontology-based   | 16,838              | 11,932                    | [ebi-ont-rules.zip](data/rules/ebi-ont-rules.zip)     |\n",
    "\n",
    "We extracted the rules from Elasticsearch using [elasticdump](https://www.npmjs.com/package/elasticdump). \n",
    "\n",
    "Commands used to export the rules and mappings from Elasticsearch to JSON format:\n",
    "\n",
    "- elasticdump --input=http://localhost:9200/cedar-rules --output=./ncbi-text-mappings.json --type=mapping\n",
    "- elasticdump --input=http://localhost:9200/cedar-rules --output=./ncbi-text-data.json --type=data\n",
    "\n",
    "Commands used to import the rules and mappings into Elasticsearch:\n",
    "\n",
    "- elasticdump --input=./ncbi-text-mappings.json --output=http://localhost:9200/cedar-rules  --type=mappings\n",
    "- elasticdump --input=./ncbi-text-data.json --output=http://localhost:9200/cedar-rules --type=data\n",
    "\n",
    "#### Some useful Elasticsearch operations\n",
    "\n",
    "Create an empty rules index in Elasticsearch: run the console command `cedarat rules-regenerateIndex`.\n",
    "\n",
    "Create a backup of the generated rules in ES:\n",
    "```json\n",
    "POST _reindex\n",
    "{\n",
    "  \"source\": {\n",
    "    \"index\": \"cedar-rules\"\n",
    "  },\n",
    "  \"dest\": {\n",
    "    \"index\": \"cedar-rules-backup\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Restore the index backup\n",
    "```json\n",
    "POST _reindex\n",
    "{\n",
    "  \"source\": {\n",
    "    \"index\": \"cedar-rules-backup\"\n",
    "  },\n",
    "  \"dest\": {\n",
    "    \"index\": \"cedar-rules-rules\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s6\"></a>Step 6: Testing\n",
    "\n",
    "In this step, we used the rules generated in the previous step to evaluate the performance of CEDAR's Value Recommender when predicting values from the test sets.\n",
    "\n",
    "Extract most common values: used for baseline recommendations\n",
    "\n",
    "\n",
    "Update:\n",
    "EVALUATION_TRAINING_DB = BIOSAMPLES_DB.NCBI\n",
    "EVALUATION_TESTING_DB = BIOSAMPLES_DB.NCBI\n",
    "EVALUATION_USE_ANNOTATED_VALUES = True\n",
    "\n",
    "Update template identifiers in the constants file:\n",
    "EVALUATION_NCBI_TEMPLATE_ID\n",
    "EVALUATION_EBI_TEMPLATE_ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Enter your CEDAR API key\n",
    "cedar_api_key = input('Please, enter you CEDAR API key and press Enter: ')\n",
    "# Run evaluation\n",
    "%run ./scripts/arm_evaluation_main.py --cedar-api-key $cedar_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"s7\"></a>Step 7: Analysis of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Links to the CSV files\n",
    "\n",
    "\n",
    "Link to the R script\n",
    "\n",
    "<img src=\"data/results/plot_MRR_2019_02_17_14_30_54.png\" alt=\"Mean Reciprocal Rank\" style=\"width: 800px;\"/>\n",
    "\n",
    "<img src=\"data/results/plot_MRR_per_field_2019_02_17_14_31_01.png\" alt=\"Mean Reciprocal Rank per field\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"additional-experiments\"></a>Additional experiments:\n",
    "\n",
    "### <a name=\"additional-experiment-1\"></a>Additional experiment 1: Confidence vs Lift\n",
    "\n",
    "Used the rules generated for the NCBI text set\n",
    "\n",
    "confidence\n",
    "confidence + support \n",
    "lift\n",
    "confidence + lift\n",
    "\n",
    "| 1st criterion | 2nd criterion | MRR (top 5) |\n",
    "|---------------|---------------|-------------|\n",
    "| confidence    | support       | 0.54        |\n",
    "| confidence    | lift          | 0.53        |\n",
    "| lift          | confidence    | 0.32        |\n",
    "| lift          | support       | 0.32        |\n",
    "\n",
    "\n",
    "### <a name=\"additional-experiment-2\"></a>Additional experiment 2: All fields\n",
    "\n",
    "NCBI template with all the fields (26)\n",
    "Min. confidence: 0.3\n",
    "Min. support: 10 instances\n",
    "Training set size: 5000 instances\n",
    "Testing set size: 500 instances\n",
    "\n",
    "\n",
    "| No. fields | No. rules generated | No. rules after filtering | Rules generation time (seg) | Mean recommendation time (ms)  | MRR   |\n",
    "|------------|---------------------|---------------------------|----------------------------|--------------------------------|-------|\n",
    "| 6          | 775                 | 572                       | 5.10                       | 43.64                          | 0.318 |\n",
    "| 26 (all)   | 233,363             | 30,559                    | 40.81                      | 44.79                          | 0.315 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"links\"></a>Links\n",
    "\n",
    "* [CEDAR's Value Recommender documentation](https://github.com/metadatacenter/cedar-docs/wiki/CEDAR-Value-Recommender)\n",
    "* [BioSample demo template](https://cedar.metadatacenter.org/instances/create/https://repo.metadatacenter.org/templates/6d9f4a83-a7ba-42be-a6af-f3cad7b2f7e3?folderId=https:%2F%2Frepo.metadatacenter.org%2Ffolders%2Fdc2ee55c-b891-4576-ba06-bfa3cf11143d)\n",
    "* [Sets of rules generated during the evaluation](#s5-results)\n",
    "* Evaluation results (.csv files) [[text-based]](data/results/text) [[ontology-based]](data/results/annotated)\n",
    "* [CEDAR User Guide](https://metadatacenter.github.io/cedar-manual/) _(in progress)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"contact\"></a>Contact\n",
    "For any questions about this notebook or about CEDAR's Value Recommender, please contact Marcos Martnez-Romero (marcosmr@stanford.edu)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
